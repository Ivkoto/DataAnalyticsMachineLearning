# Scikit-Learn
# Keras
# TensorFlow


  ### Small Datasets:

  # typically contain a few hundred to a few thousand data points. These datasets are manageable in terms of memory and computation, 
  # and training machine learning models on them is relatively fast. Small datasets are common in educational settings and some 
  # research projects.


  ### Medium-Sized Datasets:

  # Medium-sized datasets are larger than small datasets but still manageable for most standard machine learning 
  # algorithms. They often contain tens of thousands to a few hundred thousand data points. Training models on medium-sized datasets 
  # may require more computational resources and time compared to small datasets but is still feasible on standard hardware.


  ### Large Datasets:

  # Large datasets are usually characterized by their size, which can range from hundreds of thousands to millions or even billions 
  # of data points. Handling and training models on large datasets can be computationally intensive and may require distributed 
  # computing resources or specialized hardware. Large datasets are common in fields like deep learning and big data analytics.